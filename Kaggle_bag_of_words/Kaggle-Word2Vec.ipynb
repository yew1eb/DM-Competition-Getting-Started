{
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Kaggle competition : When bag of words meets bags of popcorn"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I followed the official Kaggle tutorial of the competition https://www.kaggle.com/c/word2vec-nlp-tutorial. From https://github.com/MatthieuBizien/Bag-popcorn"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "import re\n",
      "import logging\n",
      "import time\n",
      "\n",
      "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
      "    level=logging.INFO)\n",
      "\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd      \n",
      "\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "from gensim.models import Word2Vec\n",
      "\n",
      "import nltk\n",
      "# nltk.download()  # Download text data sets, including stop words\n",
      "from nltk.corpus import stopwords # Import the stop word list"
     ],
     "language": "python",
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Part 1 : simple bag of words model"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "path = \"D:/dataset/word2vec/\"\n",
      "train = pd.read_csv(path+\"labeledTrainData.tsv\", header=0, \\\n",
      "                    delimiter=\"\\t\", quoting=3)"
     ],
     "language": "python",
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " With all this stuff going down at the moment with MJ i ve started listening to his music  watching the odd documentary here and there  watched The Wiz and watched Moonwalker again  Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent  Moonwalker is part biography  part feature film which i remember going to see at the cinema when it was originally released  Some of it has subtle messages about MJ s feeling towards the press and also the obvious message of drugs are bad m kay Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring  Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him The actual feature film bit when it finally starts is only on for    minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord  Why he wants MJ dead so bad is beyond me  Because MJ overheard his plans  Nah  Joe Pesci s character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno  maybe he just hates MJ s music Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence  Also  the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene Bottom line  this movie is for people who like MJ on one level or another  which i think is most people   If not  then stay away  It does try and give off a wholesome message and ironically MJ s bestest buddy in this movie is a girl  Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty  Well  with all the attention i ve gave this subject    hmmm well i don t know because people can be different behind closed doors  i know this for a fact  He is either an extremely nice but stupid guy or one of the most sickest liars  I hope he is not the latter  \n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "F:\\DevTools\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n\nTo get rid of this warning, change this:\n\n BeautifulSoup([your markup])\n\nto this:\n\n BeautifulSoup([your markup], \"lxml\")\n\n  markup_type=markup_type))\n"
       ]
      }
     ],
     "input": [
      "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
      "                      \" \",                   # The pattern to replace it with\n",
      "                      BeautifulSoup(train[\"review\"][0])  .get_text() )  # The text to search\n",
      "print(letters_only)"
     ],
     "language": "python",
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords.zip/stopwords/' not found.  Please\n  use the NLTK Downloader to obtain the resource:  >>>\n  nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************",
        "\nDuring handling of the above exception, another exception occurred:\n",
        "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-5d80cc46c2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mF:\\DevTools\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 636\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    637\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource 'corpora/stopwords' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - 'C:\\\\Users\\\\Administrator/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\nltk_data'\n    - 'F:\\\\DevTools\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Administrator\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************"
       ]
      }
     ],
     "input": [
      "print(stopwords.words(\"english\"))"
     ],
     "language": "python",
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def review_to_words( raw_review ):\n",
      "    # Function to convert a raw review to a string of words\n",
      "    # The input is a single string (a raw movie review), and \n",
      "    # the output is a single string (a preprocessed movie review)\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(raw_review).get_text() \n",
      "    #\n",
      "    # 2. Remove non-letters        \n",
      "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
      "    #\n",
      "    # 3. Convert to lower case, split into individual words\n",
      "    words = letters_only.lower().split()                             \n",
      "    #\n",
      "    # 4. In Python, searching a set is much faster than searching\n",
      "    #   a list, so convert the stop words to a set\n",
      "    stops = set(stopwords.words(\"english\"))                  \n",
      "    # \n",
      "    # 5. Remove stop words\n",
      "    meaningful_words = [w for w in words if not w in stops]   \n",
      "    #\n",
      "    # 6. Join the words back into one string separated by space, \n",
      "    # and return the result.\n",
      "    return( \" \".join( meaningful_words ))"
     ],
     "language": "python",
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Cleaning and parsing the training set movie reviews...\n",
        "\n",
        "Review 1000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 2000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 3000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 4000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 6000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 7000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 8000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 9000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 11000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 12000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 13000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 14000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 16000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 17000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 18000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 19000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 21000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 22000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 23000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 24000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 25000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "num_reviews = train[\"review\"].size\n",
      "print(\"Cleaning and parsing the training set movie reviews...\\n\")\n",
      "clean_train_reviews = []\n",
      "for i in range( 0, num_reviews ):\n",
      "    # If the index is evenly divisible by 1000, print a message\n",
      "    if( (i+1)%1000 == 0 ):\n",
      "        print(\"Review %d of %d\" % ( i+1, num_reviews ))                                                                 \n",
      "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ))"
     ],
     "language": "python",
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(25000, 5000)\n"
       ]
      }
     ],
     "input": [
      "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
      "                             tokenizer = None,    \\\n",
      "                             preprocessor = None, \\\n",
      "                             stop_words = None,   \\\n",
      "                             max_features = 5000) \n",
      "\n",
      "# fit_transform() does two functions: First, it fits the model\n",
      "# and learns the vocabulary; second, it transforms our training data\n",
      "# into feature vectors. The input to fit_transform should be a list of \n",
      "# strings.\n",
      "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
      "\n",
      "# Numpy arrays are easy to work with, so convert the result to an \n",
      "# array\n",
      "train_data_features = train_data_features.toarray()\n",
      "print(train_data_features.shape)"
     ],
     "language": "python",
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'abc', u'abilities', u'ability', u'able', u'abraham', u'absence', u'absent', u'absolute', u'absolutely', u'absurd', u'abuse', u'abusive', u'abysmal', u'academy', u'accent', u'accents', u'accept', u'acceptable', u'accepted', u'access', u'accident', u'accidentally', u'accompanied', u'accomplished', u'according', u'account', u'accuracy', u'accurate', u'accused', u'achieve', u'achieved', u'achievement', u'acid', u'across', u'act', u'acted', u'acting', u'action', u'actions', u'activities', u'actor', u'actors', u'actress', u'actresses', u'acts', u'actual', u'actually', u'ad', u'adam', u'adams', u'adaptation', u'adaptations', u'adapted', u'add', u'added', u'adding', u'addition', u'adds', u'adequate', u'admire', u'admit', u'admittedly', u'adorable', u'adult', u'adults', u'advance', u'advanced', u'advantage', u'adventure', u'adventures', u'advertising', u'advice', u'advise', u'affair', u'affect', u'affected', u'afford', u'aforementioned', u'afraid', u'africa', u'african', u'afternoon', u'afterwards', u'age', u'aged', u'agent', u'agents', u'ages', u'aging', u'ago', u'agree', u'agreed', u'agrees', u'ah', u'ahead', u'aid', u'aids', u'aim', u'aimed']\n"
       ]
      }
     ],
     "input": [
      "# Take a look at the words in the vocabulary\n",
      "vocab = vectorizer.get_feature_names()\n",
      "print(vocab[1:100])"
     ],
     "language": "python",
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "40146 film\n",
        "26788 one\n",
        "20274 like\n",
        "15140 good\n",
        "12723 time\n",
        "12646 even\n",
        "12436 would\n",
        "11983 story\n",
        "11736 really\n",
        "11474 see\n",
        "10661 well\n",
        "9765 much\n",
        "9310 get\n",
        "9301 bad\n",
        "9285 people\n",
        "9155 also\n",
        "9061 first\n",
        "9058 great\n",
        "8362 made\n"
       ]
      }
     ],
     "input": [
      "# Sum up the counts of each vocabulary word\n",
      "dist = np.sum(train_data_features, axis=0)\n",
      "\n",
      "# For each, print the vocabulary word and the number of times it \n",
      "# appears in the training set\n",
      "for count, tag in sorted([(count, tag) for tag, count in zip(vocab, dist)], reverse=True)[1:20]:\n",
      "    print(count, tag)"
     ],
     "language": "python",
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training the random forest...\n",
        "CPU times: user 4min 29s, sys: 32.8 s, total: 5min 1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 2min 35s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "print(\"Training the random forest...\")\n",
      "# Initialize a Random Forest classifier with 100 trees\n",
      "forest = RandomForestClassifier(n_estimators = 100, n_jobs=2) \n",
      "# Fit the forest to the training set, using the bag of words as \n",
      "# features and the sentiment labels as the response variable\n",
      "#\n",
      "# This may take a few minutes to run\n",
      "forest = forest.fit( train_data_features, train[\"sentiment\"] )"
     ],
     "language": "python",
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(25000, 2)\n",
        "Cleaning and parsing the test set movie reviews...\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 25000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "# Read the test data\n",
      "test = pd.read_csv(\"testData.tsv\", header=0, delimiter=\"\\t\", \\\n",
      "                   quoting=3 )\n",
      "# Verify that there are 25,000 rows and 2 columns\n",
      "print(test.shape)\n",
      "# Create an empty list and append the clean reviews one by one\n",
      "num_reviews = len(test[\"review\"])\n",
      "clean_test_reviews = [] \n",
      "\n",
      "print(\"Cleaning and parsing the test set movie reviews...\")\n",
      "for i in range(0,num_reviews):\n",
      "    if( (i+1) % 5000 == 0 ):\n",
      "        print(\"Review %d of %d\" % (i+1, num_reviews))\n",
      "    clean_review = review_to_words( test[\"review\"][i] )\n",
      "    clean_test_reviews.append( clean_review )\n",
      "\n",
      "# Get a bag of words for the test set, and convert to a numpy array\n",
      "test_data_features = vectorizer.transform(clean_test_reviews)\n",
      "test_data_features = test_data_features.toarray()\n",
      "\n",
      "# Use the random forest to make sentiment label predictions\n",
      "result = forest.predict(test_data_features)\n",
      "\n",
      "# Copy the results to a pandas dataframe with an \"id\" column and\n",
      "# a \"sentiment\" column\n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "\n",
      "# Use pandas to write the comma-separated output file\n",
      "output.to_csv(\"Bag_of_Words_model.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score 0.84384\n"
       ]
      }
     ],
     "input": [
      "print(\"Score 0.84384\")"
     ],
     "language": "python",
     "prompt_number": 61
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Part 2 : Using Word2Vec to extract features"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "unlabeled_train = pd.read_csv( \"unlabeledTrainData.tsv\", header=0, \n",
      " delimiter=\"\\t\", quoting=3 )"
     ],
     "language": "python",
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
     ],
     "language": "python",
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def review_to_wordlist( review, remove_stopwords=False ):\n",
      "    # Function to convert a document to a sequence of words,\n",
      "    # optionally removing stop words.  Returns a list of words.\n",
      "    #\n",
      "    # 1. Remove HTML\n",
      "    review_text = BeautifulSoup(review).get_text()\n",
      "    #  \n",
      "    # 2. Remove non-letters\n",
      "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
      "    #\n",
      "    # 3. Convert words to lower case and split them\n",
      "    words = review_text.lower().split()\n",
      "    #\n",
      "    # 4. Optionally remove stop words (false by default)\n",
      "    if remove_stopwords:\n",
      "        stops = set(stopwords.words(\"english\"))\n",
      "        words = [w for w in words if not w in stops]\n",
      "    #\n",
      "    # 5. Return a list of words\n",
      "    return(words)"
     ],
     "language": "python",
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# Define a function to split a review into parsed sentences\n",
      "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
      "    # Function to split a review into parsed sentences. Returns a \n",
      "    # list of sentences, where each sentence is a list of words\n",
      "    #\n",
      "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
      "    raw_sentences = tokenizer.tokenize(review.strip())\n",
      "    #\n",
      "    # 2. Loop over each sentence\n",
      "    sentences = []\n",
      "    for raw_sentence in raw_sentences:\n",
      "        # If a sentence is empty, skip it\n",
      "        if len(raw_sentence) > 0:\n",
      "            # Otherwise, call review_to_wordlist to get a list of words\n",
      "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
      "              remove_stopwords ))\n",
      "    #\n",
      "    # Return the list of sentences (each sentence is a list of words,\n",
      "    # so this returns a list of lists\n",
      "    return sentences"
     ],
     "language": "python",
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "%%time\n",
      "sentences = []  # Initialize an empty list of sentences\n",
      "print(\"Parsing sentences from training set\")\n",
      "for i, review in enumerate(train[\"review\"]):\n",
      "    sentences += review_to_sentences(review, tokenizer)"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:182: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
        "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
        "/usr/local/lib/python2.7/site-packages/bs4/__init__.py:189: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
        "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Parsing sentences from unlabeled set\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "print(\"Parsing sentences from unlabeled set\")\n",
      "for review in unlabeled_train[\"review\"]:\n",
      "    sentences += review_to_sentences(review, tokenizer)"
     ],
     "language": "python",
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "857234\n"
       ]
      }
     ],
     "input": [
      "# Check how many sentences we have in total - should be around 850,000+\n",
      "print(len(sentences))"
     ],
     "language": "python",
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'with', u'all', u'this', u'stuff', u'going', u'down', u'at', u'the', u'moment', u'with', u'mj', u'i', u've', u'started', u'listening', u'to', u'his', u'music', u'watching', u'the', u'odd', u'documentary', u'here', u'and', u'there', u'watched', u'the', u'wiz', u'and', u'watched', u'moonwalker', u'again']\n",
        "[u'maybe', u'i', u'just', u'want', u'to', u'get', u'a', u'certain', u'insight', u'into', u'this', u'guy', u'who', u'i', u'thought', u'was', u'really', u'cool', u'in', u'the', u'eighties', u'just', u'to', u'maybe', u'make', u'up', u'my', u'mind', u'whether', u'he', u'is', u'guilty', u'or', u'innocent']\n"
       ]
      }
     ],
     "input": [
      "print(sentences[0])\n",
      "print(sentences[1])"
     ],
     "language": "python",
     "prompt_number": 37
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Training the model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced. For details on the algorithms below, see the word2vec API documentation as well as the Google documentation. \n",
      "\n",
      "* Architecture: Architecture options are skip-gram (default) or continuous bag of words. We found that skip-gram was very slightly slower but produced better results.\n",
      "* Training algorithm: Hierarchical softmax (default) or negative sampling. For us, the default worked well.\n",
      "* Downsampling of frequent words: The Google documentation recommends values between .00001 and .001. For us, values closer 0.001 seemed to improve the accuracy of the final model.\n",
      "* Word vector dimensionality: More features result in longer runtimes, and often, but not always, result in better models. Reasonable values can be in the tens to hundreds; we used 300.\n",
      "* Context / window size: How many words of context should the training algorithm take into account? 10 seems to work well for hierarchical softmax (more is better, up to a point).\n",
      "* Worker threads: Number of parallel processes to run. This is computer-specific, but between 4 and 6 should work on most systems.\n",
      "* Minimum word count: This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. Reasonable values could be between 10 and 100. In this case, since each movie occurs 30 times, we set the minimum word count to 40, to avoid attaching too much importance to individual movie titles. This resulted in an overall vocabulary size of around 15,000 words. Higher values also help limit run time."
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# Set values for various parameters\n",
      "num_features = 300    # Word vector dimensionality                      \n",
      "min_word_count = 40   # Minimum word count                        \n",
      "num_workers = 4       # Number of threads to run in parallel\n",
      "context = 10          # Context window size                                                                                    \n",
      "downsampling = 1e-3   # Downsample setting for frequent words"
     ],
     "language": "python",
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training model...\n",
        "CPU times: user 9min 33s, sys: 11.3 s, total: 9min 45s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 3min 17s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "# Initialize and train the model (this will take some time)\n",
      "print(\"Training model...\")\n",
      "model = Word2Vec(sentences, workers=num_workers, \\\n",
      "            size=num_features, min_count = min_word_count, \\\n",
      "            window = context, sample = downsampling)"
     ],
     "language": "python",
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# If you don't plan to train the model any further, calling \n",
      "# init_sims will make the model much more memory-efficient.\n",
      "model.init_sims(replace=True)"
     ],
     "language": "python"
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# It can be helpful to create a meaningful model name and \n",
      "# save the model for later use. You can load it later using Word2Vec.load()\n",
      "model_name = \"300features_40minwords_10context\"\n",
      "model.save(model_name)"
     ],
     "language": "python"
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Exploring the Model Results"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 42,
       "text": [
        "u'kitchen'"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.doesnt_match(\"man woman child kitchen\".split())"
     ],
     "language": "python",
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 43,
       "text": [
        "u'berlin'"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.doesnt_match(\"france england germany berlin\".split())"
     ],
     "language": "python",
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 44,
       "text": [
        "u'berlin'"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.doesnt_match(\"paris berlin london austria\".split())"
     ],
     "language": "python",
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 45,
       "text": [
        "[(u'woman', 0.605443000793457),\n",
        " (u'guy', 0.5038936138153076),\n",
        " (u'boy', 0.4807789623737335),\n",
        " (u'men', 0.4526759088039398),\n",
        " (u'person', 0.44328293204307556),\n",
        " (u'himself', 0.4372999668121338),\n",
        " (u'girl', 0.4281119704246521),\n",
        " (u'lady', 0.4079314172267914),\n",
        " (u'son', 0.3900514543056488),\n",
        " (u'doctor', 0.3774733543395996)]"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.most_similar(\"man\")"
     ],
     "language": "python",
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 46,
       "text": [
        "[(u'latifah', 0.4776698350906372),\n",
        " (u'victoria', 0.4730447828769684),\n",
        " (u'princess', 0.4706200659275055),\n",
        " (u'bee', 0.4386157691478729),\n",
        " (u'king', 0.4193441867828369),\n",
        " (u'prince', 0.40288692712783813),\n",
        " (u'throne', 0.394176185131073),\n",
        " (u'maria', 0.3907955586910248),\n",
        " (u'marie', 0.3768988847732544),\n",
        " (u'norma', 0.3682730197906494)]"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.most_similar(\"queen\")"
     ],
     "language": "python",
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 47,
       "text": [
        "[(u'terrible', 0.6598098278045654),\n",
        " (u'horrible', 0.6372771263122559),\n",
        " (u'dreadful', 0.6301497220993042),\n",
        " (u'atrocious', 0.5828288793563843),\n",
        " (u'horrendous', 0.5604714751243591),\n",
        " (u'abysmal', 0.5496130585670471),\n",
        " (u'laughable', 0.5353649854660034),\n",
        " (u'lousy', 0.5195952653884888),\n",
        " (u'horrid', 0.5079030990600586),\n",
        " (u'amateurish', 0.5057709217071533)]"
       ],
       "metadata": {}
      }
     ],
     "input": [
      ">>> model.most_similar(\"awful\")"
     ],
     "language": "python",
     "prompt_number": 47
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 1,
     "source": [
      "Part 3 : supervised learning using the Word2Vec features"
     ]
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Attempt 1 : Vector averaging"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def makeFeatureVec(words, model, num_features):\n",
      "    # Function to average all of the word vectors in a given\n",
      "    # paragraph\n",
      "    #\n",
      "    # Pre-initialize an empty numpy array (for speed)\n",
      "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
      "    #\n",
      "    nwords = 0.\n",
      "    # \n",
      "    # Index2word is a list that contains the names of the words in \n",
      "    # the model's vocabulary. Convert it to a set, for speed \n",
      "    index2word_set = set(model.index2word)\n",
      "    #\n",
      "    # Loop over each word in the review and, if it is in the model's\n",
      "    # vocaublary, add its feature vector to the total\n",
      "    for word in words:\n",
      "        if word in index2word_set: \n",
      "            nwords = nwords + 1.\n",
      "            featureVec = np.add(featureVec,model[word])\n",
      "    # \n",
      "    # Divide the result by the number of words to get the average\n",
      "    featureVec = np.divide(featureVec,nwords)\n",
      "    return featureVec\n",
      "\n",
      "\n",
      "def getAvgFeatureVecs(reviews, model, num_features):\n",
      "    # Given a set of reviews (each one a list of words), calculate \n",
      "    # the average feature vector for each one and return a 2D numpy array \n",
      "    # \n",
      "    # Initialize a counter\n",
      "    counter = 0.\n",
      "    # \n",
      "    # Preallocate a 2D numpy array, for speed\n",
      "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
      "    # \n",
      "    # Loop through the reviews\n",
      "    for review in reviews:\n",
      "       #\n",
      "       # Print a status message every 1000th review\n",
      "       if counter%5000. == 0.:\n",
      "           print(\"Review %d of %d\" % (counter, len(reviews)))\n",
      "       # \n",
      "       # Call the function (defined above) that makes average feature vectors\n",
      "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
      "           num_features)\n",
      "       #\n",
      "       # Increment the counter\n",
      "       counter = counter + 1.\n",
      "    return reviewFeatureVecs"
     ],
     "language": "python",
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 18.8 s, sys: 1.23 s, total: 20.1 s\n",
        "Wall time: 20.2 s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "clean_train_reviews = []\n",
      "for review in train[\"review\"]:\n",
      "    clean_train_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))"
     ],
     "language": "python",
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 1min 18s, sys: 582 ms, total: 1min 19s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1min 19s\n"
       ]
      }
     ],
     "input": [
      "%time trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )"
     ],
     "language": "python",
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Creating average feature vecs for test reviews\n",
        "CPU times: user 20.7 s, sys: 1.8 s, total: 22.5 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 22.7 s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "print(\"Creating average feature vecs for test reviews\")\n",
      "clean_test_reviews = []\n",
      "for review in test[\"review\"]:\n",
      "    clean_test_reviews.append( review_to_wordlist( review, \\\n",
      "        remove_stopwords=True ))"
     ],
     "language": "python",
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Review 0 of 25000\n",
        "Review 5000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 10000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 15000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Review 20000 of 25000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "CPU times: user 1min 18s, sys: 560 ms, total: 1min 19s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 1min 19s\n"
       ]
      }
     ],
     "input": [
      "%time testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
     ],
     "language": "python",
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting a random forest to labeled training data...\n",
        "CPU times: user 1min 3s, sys: 766 ms, total: 1min 3s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 32.4 s\n",
        "CPU times: user 1.08 s, sys: 25.8 ms, total: 1.11 s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 768 ms\n"
       ]
      }
     ],
     "input": [
      "forest = RandomForestClassifier( n_estimators = 100, n_jobs=2)\n",
      "\n",
      "print(\"Fitting a random forest to labeled training data...\")\n",
      "%time forest = forest.fit( trainDataVecs, train[\"sentiment\"] )\n",
      "\n",
      "# Test & extract results \n",
      "result = forest.predict( testDataVecs )\n",
      "\n",
      "# Write the test results \n",
      "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
      "output.to_csv( \"Word2Vec_AverageVectors.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Score 0.83248\n"
       ]
      }
     ],
     "input": [
      "print(\"Score 0.83248\")"
     ],
     "language": "python",
     "prompt_number": 60
    },
    {
     "cell_type": "heading",
     "metadata": {},
     "level": 2,
     "source": [
      "Attempt 2 : clustering"
     ]
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 33min 29s, sys: 1min, total: 34min 30s\n",
        "Wall time: 17min 41s\n"
       ]
      }
     ],
     "input": [
      "%%time\n",
      "# Set \"k\" (num_clusters) to be 1/5th of the vocabulary size, or an\n",
      "# average of 5 words per cluster\n",
      "word_vectors = model.syn0\n",
      "num_clusters = word_vectors.shape[0] / 5\n",
      "\n",
      "# Initalize a k-means object and use it to extract centroids\n",
      "kmeans_clustering = KMeans( n_clusters = num_clusters )\n",
      "idx = kmeans_clustering.fit_predict( word_vectors )"
     ],
     "language": "python",
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "word_centroid_map = dict(zip( model.index2word, idx ))"
     ],
     "language": "python",
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Cluster 0\n",
        "[u'pacific']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 1\n",
        "[u'mutilated', u'screwing', u'shaken', u'slashed', u'knocked', u'punched', u'choking', u'yelled', u'fixing', u'shaved', u'melted', u'chopped', u'dumped', u'whacked', u'miraculously', u'waking', u'crushed', u'choked', u'sliced', u'drowned', u'bumped', u'handing', u'tracked', u'strangled', u'hypnotized', u'stripped', u'cooked', u'hammered', u'contaminated', u'busted', u'slammed', u'drugged', u'blasted', u'gunned', u'burnt', u'hung', u'swallowed', u'hacked', u'confessed', u'cleared', u'coma', u'cleaned', u'interrupted', u'bashed', u'gutted']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 2\n",
        "[u'hilariously', u'shockingly', u'dreadfully', u'downright', u'amazingly', u'laughably', u'ridiculously', u'unbelievably', u'awfully']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 3\n",
        "[u'maintained', u'sustained']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 4\n",
        "[u'plunges', u'slices', u'boiling', u'cloak', u'dives', u'rises', u'melts', u'erupts', u'tosses', u'sinks', u'entrance', u'recovers', u'switches', u'piling', u'staircase', u'explosive', u'lays', u'sweeps', u'pushes', u'hangs', u'stalk', u'drowns', u'pounding', u'ensue', u'dissolves', u'crosses', u'emerges', u'descends', u'transforming', u'spills', u'dagger', u'transforms', u'drifts', u'injected', u'reaches', u'finishes', u'splits', u'settles', u'swings', u'looses', u'degenerates', u'ensues', u'wanders', u'stumbles', u'bursts', u'rails', u'pours', u'slides', u'closes', u'slips']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 5\n",
        "[u'recalled']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 6\n",
        "[u'artistic', u'seamless', u'cinematographic', u'placement', u'transition', u'styles', u'aesthetics', u'stylistic', u'enhance', u'filmmaking', u'artistry', u'medium', u'realism', u'coupled', u'style', u'storytelling']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 7\n",
        "[u'thornton', u'fosse', u'hoskins']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 8\n",
        "[u'largest']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Cluster 9\n",
        "[u'incompetent', u'ham', u'plotting', u'dire', u'inept', u'diabolical', u'fisted', u'woeful', u'pitiful', u'sloppy', u'dismal']"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "input": [
      "# For the first 10 clusters\n",
      "for cluster in xrange(0,10):\n",
      "    #\n",
      "    # Print the cluster number  \n",
      "    print(\"\\nCluster %d\" % cluster)\n",
      "    #\n",
      "    # Find all of the words for that cluster number, and print them out\n",
      "    words = []\n",
      "    for i in xrange(0,len(word_centroid_map.values())):\n",
      "        if( word_centroid_map.values()[i] == cluster ):\n",
      "            words.append(word_centroid_map.keys()[i])\n",
      "    print(words)"
     ],
     "language": "python",
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "def create_bag_of_centroids( wordlist, word_centroid_map ):\n",
      "    #\n",
      "    # The number of clusters is equal to the highest cluster index\n",
      "    # in the word / centroid map\n",
      "    num_centroids = max( word_centroid_map.values() ) + 1\n",
      "    #\n",
      "    # Pre-allocate the bag of centroids vector (for speed)\n",
      "    bag_of_centroids = np.zeros( num_centroids, dtype=\"float32\" )\n",
      "    #\n",
      "    # Loop over the words in the review. If the word is in the vocabulary,\n",
      "    # find which cluster it belongs to, and increment that cluster count \n",
      "    # by one\n",
      "    for word in wordlist:\n",
      "        if word in word_centroid_map:\n",
      "            index = word_centroid_map[word]\n",
      "            bag_of_centroids[index] += 1\n",
      "    #\n",
      "    # Return the \"bag of centroids\"\n",
      "    return bag_of_centroids"
     ],
     "language": "python",
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      "# Pre-allocate an array for the training set bags of centroids (for speed)\n",
      "train_centroids = np.zeros( (train[\"review\"].size, num_clusters), \\\n",
      "    dtype=\"float32\" )\n",
      "\n",
      "# Transform the training set reviews into bags of centroids\n",
      "counter = 0\n",
      "for review in clean_train_reviews:\n",
      "    train_centroids[counter] = create_bag_of_centroids( review, \\\n",
      "        word_centroid_map )\n",
      "    counter += 1\n",
      "\n",
      "# Repeat for test reviews \n",
      "test_centroids = np.zeros(( test[\"review\"].size, num_clusters), \\\n",
      "    dtype=\"float32\" )\n",
      "\n",
      "counter = 0\n",
      "for review in clean_test_reviews:\n",
      "    test_centroids[counter] = create_bag_of_centroids( review, \\\n",
      "        word_centroid_map )\n",
      "    counter += 1"
     ],
     "language": "python",
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting a random forest to labeled training data...\n"
       ]
      }
     ],
     "input": [
      "# Fit a random forest and extract predictions \n",
      "forest = RandomForestClassifier(n_estimators = 100)\n",
      "\n",
      "# Fitting the forest may take a few minutes\n",
      "print(\"Fitting a random forest to labeled training data...\")\n",
      "forest = forest.fit(train_centroids,train[\"sentiment\"])\n",
      "result = forest.predict(test_centroids)\n",
      "\n",
      "# Write the test results \n",
      "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
      "output.to_csv( \"BagOfCentroids.csv\", index=False, quoting=3 )"
     ],
     "language": "python",
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "score 0.84648\n"
       ]
      }
     ],
     "input": [
      "print(\"score 0.84648\")"
     ],
     "language": "python",
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "execution_count": null,
     "metadata": {},
     "outputs": [],
     "input": [
      ""
     ],
     "language": "python"
    }
   ]
  }
 ],
 "cells": [],
 "metadata": {
  "name": "",
  "signature": "sha256:40d6589b3714ef0f9b71d45a3bac9f37360316b44b235303e8fc72d1bd8cc66c"
 },
 "nbformat": 3,
 "nbformat_minor": 0
}